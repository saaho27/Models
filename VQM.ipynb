{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:01.905279Z",
     "iopub.status.busy": "2024-07-23T09:54:01.904782Z",
     "iopub.status.idle": "2024-07-23T09:54:01.911689Z",
     "shell.execute_reply": "2024-07-23T09:54:01.910648Z",
     "shell.execute_reply.started": "2024-07-23T09:54:01.905253Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.layers import Dense, Layer, Embedding, GRU, Bidirectional, LSTM, GlobalAveragePooling2D , Dropout\n",
    "from tensorflow.keras.utils import Sequence \n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd \n",
    "from tqdm.notebook  import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:07.532158Z",
     "iopub.status.busy": "2024-07-23T09:54:07.531785Z",
     "iopub.status.idle": "2024-07-23T09:54:07.777969Z",
     "shell.execute_reply": "2024-07-23T09:54:07.777173Z",
     "shell.execute_reply.started": "2024-07-23T09:54:07.532131Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/datasett/combined_qa(2).txt\" , 'r') as file:\n",
    "    text = file.read()\n",
    "text = text.split('\\n')\n",
    "tokenizer = Tokenizer(filters = '' , oov_token= '<OOV>')\n",
    "tokenizer.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:07.779586Z",
     "iopub.status.busy": "2024-07-23T09:54:07.779290Z",
     "iopub.status.idle": "2024-07-23T09:54:07.784499Z",
     "shell.execute_reply": "2024-07-23T09:54:07.783561Z",
     "shell.execute_reply.started": "2024-07-23T09:54:07.779560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 4, 6, 9, 3]]\n"
     ]
    }
   ],
   "source": [
    "sample_questions = ' answer of this question is '\n",
    "sequece_tokens = tokenizer.texts_to_sequences([sample_questions])\n",
    "print(sequece_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:07.985379Z",
     "iopub.status.busy": "2024-07-23T09:54:07.985081Z",
     "iopub.status.idle": "2024-07-23T09:54:08.154605Z",
     "shell.execute_reply": "2024-07-23T09:54:08.153822Z",
     "shell.execute_reply.started": "2024-07-23T09:54:07.985353Z"
    }
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:08.205937Z",
     "iopub.status.busy": "2024-07-23T09:54:08.205575Z",
     "iopub.status.idle": "2024-07-23T09:54:08.210539Z",
     "shell.execute_reply": "2024-07-23T09:54:08.209395Z",
     "shell.execute_reply.started": "2024-07-23T09:54:08.205908Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:08.511440Z",
     "iopub.status.busy": "2024-07-23T09:54:08.511035Z",
     "iopub.status.idle": "2024-07-23T09:54:08.517001Z",
     "shell.execute_reply": "2024-07-23T09:54:08.516017Z",
     "shell.execute_reply.started": "2024-07-23T09:54:08.511407Z"
    }
   },
   "outputs": [],
   "source": [
    "start_token = len(word_index) + 1\n",
    "end_token = len(word_index) + 2\n",
    "tokenizer.word_index['start'] = start_token\n",
    "tokenizer.word_index['end']  = end_token\n",
    "tokenizer.index_word[start_token] = 'start'\n",
    "tokenizer.index_word[end_token]  = 'end'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:08.652779Z",
     "iopub.status.busy": "2024-07-23T09:54:08.652430Z",
     "iopub.status.idle": "2024-07-23T09:54:08.660288Z",
     "shell.execute_reply": "2024-07-23T09:54:08.659387Z",
     "shell.execute_reply.started": "2024-07-23T09:54:08.652753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3134, 3135, 'start', 'end')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['start'] , tokenizer.word_index['end'] , tokenizer.index_word[3134] , tokenizer.index_word[3135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:08.884034Z",
     "iopub.status.busy": "2024-07-23T09:54:08.883214Z",
     "iopub.status.idle": "2024-07-23T09:54:08.899610Z",
     "shell.execute_reply": "2024-07-23T09:54:08.898624Z",
     "shell.execute_reply.started": "2024-07-23T09:54:08.883995Z"
    }
   },
   "outputs": [],
   "source": [
    "sequences_with_se = [[start_token] + seq + [end_token]  for seq in sequences ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:09.090110Z",
     "iopub.status.busy": "2024-07-23T09:54:09.089793Z",
     "iopub.status.idle": "2024-07-23T09:54:09.095047Z",
     "shell.execute_reply": "2024-07-23T09:54:09.094148Z",
     "shell.execute_reply.started": "2024-07-23T09:54:09.090084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 3, 85, 2, 2, 521, 37, 66, 48, 145, 446, 5, 2, 327, 7, 8, 4, 6, 9, 3, 26]\n"
     ]
    }
   ],
   "source": [
    "se = tokenizer.texts_to_sequences([\"what is between the the two white and black garbage bins in the image1 ? answer of this question is  chair\"])[0]\n",
    "print(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:09.319703Z",
     "iopub.status.busy": "2024-07-23T09:54:09.319040Z",
     "iopub.status.idle": "2024-07-23T09:54:09.326187Z",
     "shell.execute_reply": "2024-07-23T09:54:09.325088Z",
     "shell.execute_reply.started": "2024-07-23T09:54:09.319674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'is',\n",
       " 'between',\n",
       " 'the',\n",
       " 'the',\n",
       " 'two',\n",
       " 'white',\n",
       " 'and',\n",
       " 'black',\n",
       " 'garbage',\n",
       " 'bins',\n",
       " 'in',\n",
       " 'the',\n",
       " 'image1',\n",
       " '?',\n",
       " 'answer',\n",
       " 'of',\n",
       " 'this',\n",
       " 'question',\n",
       " 'is',\n",
       " 'chair']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.index_word[idx] for idx in se]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:09.534916Z",
     "iopub.status.busy": "2024-07-23T09:54:09.534565Z",
     "iopub.status.idle": "2024-07-23T09:54:09.540565Z",
     "shell.execute_reply": "2024-07-23T09:54:09.539721Z",
     "shell.execute_reply.started": "2024-07-23T09:54:09.534888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12469"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences_with_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:09.749090Z",
     "iopub.status.busy": "2024-07-23T09:54:09.748767Z",
     "iopub.status.idle": "2024-07-23T09:54:09.756017Z",
     "shell.execute_reply": "2024-07-23T09:54:09.755109Z",
     "shell.execute_reply.started": "2024-07-23T09:54:09.749065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_len = max(len(seq) for seq in sequences_with_se)\n",
    "pad_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:09.958959Z",
     "iopub.status.busy": "2024-07-23T09:54:09.958668Z",
     "iopub.status.idle": "2024-07-23T09:54:10.032522Z",
     "shell.execute_reply": "2024-07-23T09:54:10.031640Z",
     "shell.execute_reply.started": "2024-07-23T09:54:09.958934Z"
    }
   },
   "outputs": [],
   "source": [
    "padded_sequence = pad_sequences(sequences_with_se ,maxlen=pad_len ,padding= 'post' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:10.182583Z",
     "iopub.status.busy": "2024-07-23T09:54:10.182277Z",
     "iopub.status.idle": "2024-07-23T09:54:10.187188Z",
     "shell.execute_reply": "2024-07-23T09:54:10.186120Z",
     "shell.execute_reply.started": "2024-07-23T09:54:10.182558Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.index_word[0] = '<pad>'\n",
    "tokenizer.word_index['<pad>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:10.410873Z",
     "iopub.status.busy": "2024-07-23T09:54:10.410180Z",
     "iopub.status.idle": "2024-07-23T09:54:10.418683Z",
     "shell.execute_reply": "2024-07-23T09:54:10.417747Z",
     "shell.execute_reply.started": "2024-07-23T09:54:10.410842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start',\n",
       " 'what',\n",
       " 'is',\n",
       " 'on',\n",
       " 'the',\n",
       " 'left',\n",
       " 'side',\n",
       " 'of',\n",
       " 'the',\n",
       " 'white',\n",
       " 'oven',\n",
       " 'on',\n",
       " 'the',\n",
       " 'floor',\n",
       " 'and',\n",
       " 'on',\n",
       " 'right',\n",
       " 'side',\n",
       " 'of',\n",
       " 'the',\n",
       " 'blue',\n",
       " 'armchair',\n",
       " 'in',\n",
       " 'the',\n",
       " 'image1',\n",
       " '?',\n",
       " 'answer',\n",
       " 'of',\n",
       " 'this',\n",
       " 'question',\n",
       " 'is',\n",
       " 'garbage_bin',\n",
       " 'end',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.index_word[seq] for seq in padded_sequence[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:10.626256Z",
     "iopub.status.busy": "2024-07-23T09:54:10.625734Z",
     "iopub.status.idle": "2024-07-23T09:54:10.631118Z",
     "shell.execute_reply": "2024-07-23T09:54:10.630197Z",
     "shell.execute_reply.started": "2024-07-23T09:54:10.626226Z"
    }
   },
   "outputs": [],
   "source": [
    "padded_sequence = np.array(padded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:10.840437Z",
     "iopub.status.busy": "2024-07-23T09:54:10.840127Z",
     "iopub.status.idle": "2024-07-23T09:54:10.848799Z",
     "shell.execute_reply": "2024-07-23T09:54:10.847894Z",
     "shell.execute_reply.started": "2024-07-23T09:54:10.840403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3134,   10,    3,   11,    2,   19,   15,    4,    2,   37,  139,\n",
       "          11,    2,   33,   66,   11,   18,   15,    4,    2,   71,  351,\n",
       "           5,    2,  327,    7,    8,    4,    6,    9,    3,   77, 3135,\n",
       "           0,    0,    0,    0,    0,    0],\n",
       "       [3134,   10,    3,   11,    2,   19,   15,    4,    2, 1001, 1637,\n",
       "          66,   11,    2,   18,   15,    4,    2,   26,    5,    2,  327,\n",
       "           7,    8,    4,    6,    9,    3,   13, 3135,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0],\n",
       "       [3134,   10,    3,   85,    2,    2,  521,   37,   66,   48,  145,\n",
       "         446,    5,    2,  327,    7,    8,    4,    6,    9,    3,   26,\n",
       "        3135,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0],\n",
       "       [3134,   16,   17,   35,   12,   85,    2, 1001, 1637,   66,    2,\n",
       "          37,  139,   11,    2,   33,    5,    2,  327,    7,    8,    4,\n",
       "           6,    9,    3,   45, 3135,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequence[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:11.027714Z",
     "iopub.status.busy": "2024-07-23T09:54:11.027415Z",
     "iopub.status.idle": "2024-07-23T09:54:11.032643Z",
     "shell.execute_reply": "2024-07-23T09:54:11.031722Z",
     "shell.execute_reply.started": "2024-07-23T09:54:11.027681Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(sequence, tokenizer , pad_len):\n",
    "    tokens = tokenizer.texts_to_sequences([sequence])[0]\n",
    "    padded_sequence = pad_sequences([tokens] ,maxlen=pad_len ,padding= 'post' )[0]\n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:11.254434Z",
     "iopub.status.busy": "2024-07-23T09:54:11.254158Z",
     "iopub.status.idle": "2024-07-23T09:54:11.260784Z",
     "shell.execute_reply": "2024-07-23T09:54:11.259848Z",
     "shell.execute_reply.started": "2024-07-23T09:54:11.254411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10,   3,  85,   2,   2, 521,  37,  66,  48, 145, 446,   5,   2,\n",
       "       327,   7,   8,   4,   6,   9,   3,  26,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = \"what is between the the two white and black garbage bins in the image1 ? answer of this question is  chair\"\n",
    "tokenize(seq , tokenizer , pad_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:11.469321Z",
     "iopub.status.busy": "2024-07-23T09:54:11.468721Z",
     "iopub.status.idle": "2024-07-23T09:54:11.484753Z",
     "shell.execute_reply": "2024-07-23T09:54:11.483739Z",
     "shell.execute_reply.started": "2024-07-23T09:54:11.469287Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, dataframe, tokenizer, pad_len, batch_size=32, new_shape=(224, 224), shuffle=True):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.shuffle = shuffle\n",
    "        self.pad_len = pad_len\n",
    "        self.batch_size = batch_size\n",
    "        self.new_shape = new_shape\n",
    "        self.indexes = np.arange(len(dataframe))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.dataframe) / self.batch_size))\n",
    "                   \n",
    "    def __getitem__(self, index):\n",
    "        start_index = index * self.batch_size \n",
    "        end_index = (index + 1) * self.batch_size \n",
    "        batch_indexes = self.indexes[start_index:end_index]\n",
    "        batch_images, batch_texts, batch_decoder_input, batch_decoder_target = [], [], [], []\n",
    "        \n",
    "        for idx in batch_indexes:\n",
    "            row = self.dataframe.iloc[idx]\n",
    "            image_id = row['image_id']\n",
    "            image_path = f\"/kaggle/input/visual-question-answering-computer-vision-nlp/dataset/images/{image_id}.png\"     \n",
    "            question = row['question']\n",
    "            answer = row['answer']\n",
    "            image_data = cv2.imread(image_path)\n",
    "            image_data = cv2.resize(image_data, self.new_shape)\n",
    "            image_data = image_data / 255.0  \n",
    "            \n",
    "            text = f\"start {question} answer of this question is {answer} end\"\n",
    "            tokenized_text = self.tokenize(text)\n",
    "            \n",
    "            decoder_input = tokenized_text[:-1]\n",
    "            decoder_target = tokenized_text[1:]\n",
    "            \n",
    "            batch_images.append(image_data)\n",
    "            batch_texts.append(tokenized_text)\n",
    "            batch_decoder_input.append(decoder_input)\n",
    "            batch_decoder_target.append(decoder_target)\n",
    "            \n",
    "        batch_images = np.array(batch_images, dtype=np.float32)\n",
    "        batch_decoder_input = np.array(batch_decoder_input, dtype=np.int32)\n",
    "        batch_decoder_target = np.array(batch_decoder_target, dtype=np.int32)\n",
    "        \n",
    "        encoder_input = tf.convert_to_tensor(batch_images, dtype=tf.float32)\n",
    "        decoder_input = tf.convert_to_tensor(batch_decoder_input, dtype=tf.int32)\n",
    "        decoder_target = tf.convert_to_tensor(batch_decoder_target, dtype=tf.int32)\n",
    "        \n",
    "        return (encoder_input, decoder_input), decoder_target\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def tokenize(self, sequence):\n",
    "        tokens = self.tokenizer.texts_to_sequences([sequence])[0]\n",
    "        padded_sequence = pad_sequences([tokens], maxlen=self.pad_len, padding='post')[0]\n",
    "        return padded_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:11.677393Z",
     "iopub.status.busy": "2024-07-23T09:54:11.676794Z",
     "iopub.status.idle": "2024-07-23T09:54:11.741031Z",
     "shell.execute_reply": "2024-07-23T09:54:11.740159Z",
     "shell.execute_reply.started": "2024-07-23T09:54:11.677358Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/visual-question-answering-computer-vision-nlp/dataset/data_train.csv\")\n",
    "val_df = pd.read_csv(\"/kaggle/input/visual-question-answering-computer-vision-nlp/dataset/data_eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:11.862894Z",
     "iopub.status.busy": "2024-07-23T09:54:11.862294Z",
     "iopub.status.idle": "2024-07-23T09:54:11.866951Z",
     "shell.execute_reply": "2024-07-23T09:54:11.865986Z",
     "shell.execute_reply.started": "2024-07-23T09:54:11.862865Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = CustomDataGenerator(train_df , tokenizer , pad_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:12.084201Z",
     "iopub.status.busy": "2024-07-23T09:54:12.083887Z",
     "iopub.status.idle": "2024-07-23T09:54:13.514970Z",
     "shell.execute_reply": "2024-07-23T09:54:13.513910Z",
     "shell.execute_reply.started": "2024-07-23T09:54:12.084176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3)\n",
      "(32, 38)\n",
      "(32, 38)\n"
     ]
    }
   ],
   "source": [
    "for inputs , target in train_data:\n",
    "    print(inputs[0].shape)\n",
    "    print(inputs[1].shape)\n",
    "    print(target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:14.204715Z",
     "iopub.status.busy": "2024-07-23T09:54:14.204061Z",
     "iopub.status.idle": "2024-07-23T09:54:14.210856Z",
     "shell.execute_reply": "2024-07-23T09:54:14.209843Z",
     "shell.execute_reply.started": "2024-07-23T09:54:14.204679Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def detokenize_sequence( tokenizer,sequence):\n",
    "    word_index = tokenizer.word_index\n",
    "    index_to_word = {idx: word for word, idx in word_index.items()}\n",
    "    \n",
    "    if isinstance(sequence, tf.Tensor):\n",
    "        sequence = sequence.numpy()\n",
    "    \n",
    "    # Ensure sequence is 1-dimensional\n",
    "    sequence = sequence.flatten()\n",
    "\n",
    "    text = \" \".join([index_to_word.get(int(token), '') for token in sequence if int(token) != 0])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:54:15.375859Z",
     "iopub.status.busy": "2024-07-23T09:54:15.374870Z",
     "iopub.status.idle": "2024-07-23T09:54:15.800974Z",
     "shell.execute_reply": "2024-07-23T09:54:15.800084Z",
     "shell.execute_reply.started": "2024-07-23T09:54:15.375822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image input (32, 224, 224, 3)\n",
      "Encoder Input : ['start what object is stuck on the right storage rack answer of this question is tissue_roll end', 'start what object is found on the right top answer of this question is oven end', 'start what is on the left side of the monitor answer of this question is remote_control end', 'start what objects are found in front of the table answer of this question is books, toy, photo end', 'start what are the dark brown objects in this picture answer of this question is piano, piano_bench end', 'start what is to the right of the piano answer of this question is stroller end', 'start what is the object on the floor close to the wall divider answer of this question is stool end', 'start what is on the table answer of this question is tablecloth end', 'start what is found in front the man on the left side answer of this question is door end', 'start what is on the flush tank answer of this question is tissue_box end', 'start what is colour of television answer of this question is black end', 'start what is on the table answer of this question is book, paper, decorative_item end', 'start what is the colour of the poster in the right side answer of this question is purple end', 'start what is found on the corner of the room answer of this question is lamp end', 'start what is to the right of the game table answer of this question is table end', 'start what object is found on the table answer of this question is coffee_machine end', 'start what is in the sink answer of this question is plate, spatula end', 'start how many lamps are above the cash desk answer of this question is 2 end', 'start how many colours fully displayed in doll answer of this question is 3 end', 'start what is opposite to the bookshelf answer of this question is table end', 'start what is in front of the window answer of this question is sofa end', 'start what is placed above the table answer of this question is bag end', 'start what is attached to the table through the wiring answer of this question is lamp end', 'start what is behind the chair answer of this question is books end', 'start what is found to the right side of the bed answer of this question is cabinet end', 'start what colour shirt is the man wearing answer of this question is black end', 'start what is on the right side of the toilet on the floor answer of this question is magazine_holder end', 'start what is on the table answer of this question is books end', 'start what is on the table answer of this question is books, paper end', 'start what is to the right of computer stand answer of this question is speaker end', 'start what is in front of the door answer of this question is table end', 'start how many small drawers are there answer of this question is 11 end']\n",
      "\n",
      "\n",
      "Encoder Output : ['what object is stuck on the right storage rack answer of this question is tissue_roll end', 'what object is found on the right top answer of this question is oven end', 'what is on the left side of the monitor answer of this question is remote_control end', 'what objects are found in front of the table answer of this question is books, toy, photo end', 'what are the dark brown objects in this picture answer of this question is piano, piano_bench end', 'what is to the right of the piano answer of this question is stroller end', 'what is the object on the floor close to the wall divider answer of this question is stool end', 'what is on the table answer of this question is tablecloth end', 'what is found in front the man on the left side answer of this question is door end', 'what is on the flush tank answer of this question is tissue_box end', 'what is colour of television answer of this question is black end', 'what is on the table answer of this question is book, paper, decorative_item end', 'what is the colour of the poster in the right side answer of this question is purple end', 'what is found on the corner of the room answer of this question is lamp end', 'what is to the right of the game table answer of this question is table end', 'what object is found on the table answer of this question is coffee_machine end', 'what is in the sink answer of this question is plate, spatula end', 'how many lamps are above the cash desk answer of this question is 2 end', 'how many colours fully displayed in doll answer of this question is 3 end', 'what is opposite to the bookshelf answer of this question is table end', 'what is in front of the window answer of this question is sofa end', 'what is placed above the table answer of this question is bag end', 'what is attached to the table through the wiring answer of this question is lamp end', 'what is behind the chair answer of this question is books end', 'what is found to the right side of the bed answer of this question is cabinet end', 'what colour shirt is the man wearing answer of this question is black end', 'what is on the right side of the toilet on the floor answer of this question is magazine_holder end', 'what is on the table answer of this question is books end', 'what is on the table answer of this question is books, paper end', 'what is to the right of computer stand answer of this question is speaker end', 'what is in front of the door answer of this question is table end', 'how many small drawers are there answer of this question is 11 end']\n"
     ]
    }
   ],
   "source": [
    "for inputs , targets in train_data:\n",
    "    print(\"Image input\", inputs[0].shape)\n",
    "    encoder_input = [detokenize_sequence(tokenizer , ip) for ip in inputs[1]]\n",
    "    print(\"Encoder Input :\", encoder_input)\n",
    "    print(\"\\n\")\n",
    "    target = [detokenize_sequence(tokenizer , tar) for tar in targets]\n",
    "    print(\"Encoder Output :\" ,target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:22:58.015964Z",
     "iopub.status.busy": "2024-07-23T12:22:58.015441Z",
     "iopub.status.idle": "2024-07-23T12:22:58.046792Z",
     "shell.execute_reply": "2024-07-23T12:22:58.045861Z",
     "shell.execute_reply.started": "2024-07-23T12:22:58.015923Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageEncoder(tf.keras.Model):\n",
    "    def __init__(self , input_shape):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=self.input_shape)\n",
    "        self.base_model.trainable = False \n",
    "        self.model = tf.keras.Sequential([\n",
    "            self.base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dropout(0.5),\n",
    "            Dense(256, activation='relu')\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.model(x)\n",
    "        return x \n",
    "    \n",
    "    \n",
    "class LanguageEncoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, emb_dim, rnn_units):\n",
    "        super(LanguageEncoder, self).__init__()\n",
    "        self.embedding = Embedding(input_dim=vocab_size, output_dim=emb_dim)\n",
    "        self.rnn = Bidirectional(LSTM(rnn_units, return_sequences=True))\n",
    "\n",
    "    def call(self, text):\n",
    "        emb = self.embedding(text)\n",
    "        output = self.rnn(emb)\n",
    "        return output \n",
    "\n",
    "\n",
    "# Attention mechanism\n",
    "# Additive attention are used\n",
    "class InsampleAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(InsampleAttention, self).__init__()\n",
    "        self.w1 = Dense(units)\n",
    "        self.w2 = Dense(units)\n",
    "        self.v = Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        query = tf.expand_dims(query, axis=1)\n",
    "        score = self.v(tf.nn.tanh(self.w1(query) + self.w2(values)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class CrossSampleAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(CrossSampleAttention, self).__init__()\n",
    "        self.w1 = Dense(units)\n",
    "        self.w2 = Dense(units)\n",
    "        self.v = Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        values = tf.expand_dims(values, axis=1)\n",
    "        \n",
    "        query = self.w1(query)\n",
    "        value = self.w2(values)\n",
    "\n",
    "        score = self.v(tf.nn.tanh(query + value))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, emb_dim, rnn_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, emb_dim)\n",
    "        self.rnn = LSTM(rnn_units, return_sequences=True, return_state=True)\n",
    "        self.dense = Dense(vocab_size)\n",
    "        \n",
    "    def call(self, context_vector, target_sequence, training=False):\n",
    "\n",
    "        target_emb = self.embedding(target_sequence)\n",
    "        context_vector = tf.expand_dims(context_vector, axis=1)\n",
    "    \n",
    "        context_vector = tf.cast(context_vector, dtype=tf.float32)\n",
    "\n",
    "        context_vector = tf.tile(context_vector, [1, tf.shape(target_sequence)[1], 1])\n",
    "        rnn_input = tf.concat([target_emb, context_vector], axis=-1)\n",
    "        rnn_output, _, _ = self.rnn(rnn_input, training=training)\n",
    "        logits = self.dense(rnn_output)\n",
    "        logits = logits[:, :38, :] \n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "class VQAModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, emb_dim, rnn_units):\n",
    "        super(VQAModel, self).__init__()\n",
    "        self.image_encoder = ImageEncoder(input_shape=(224, 224, 3)) \n",
    "        self.language_encoder = LanguageEncoder(vocab_size, emb_dim, rnn_units)\n",
    "        self.insample_attention = InsampleAttention(rnn_units)\n",
    "        self.cross_attention = CrossSampleAttention(rnn_units)\n",
    "        self.decoder = Decoder(vocab_size, emb_dim, rnn_units)\n",
    "        self.dense = Dense(vocab_size)\n",
    "\n",
    "    def call(self, images, texts, training=False):\n",
    "        image_feature = self.image_encoder(images)\n",
    "        text_feature = self.language_encoder(texts)\n",
    "        \n",
    "        context_vector_is, _ = self.insample_attention(image_feature, text_feature)\n",
    "        context_vector_cs, _ = self.cross_attention(text_feature, image_feature)\n",
    "\n",
    "        concat_context = tf.concat([context_vector_is, context_vector_cs], axis=-1)\n",
    "        \n",
    "        return concat_context\n",
    "\n",
    "    def decode(self, context_vector, target_sequence, training=False):\n",
    "        # Ensure target_sequence is properly formatted for the decoder\n",
    "        return self.decoder(target_sequence, context_vector, training=training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:22:58.711062Z",
     "iopub.status.busy": "2024-07-23T12:22:58.710379Z",
     "iopub.status.idle": "2024-07-23T12:22:59.883124Z",
     "shell.execute_reply": "2024-07-23T12:22:59.882181Z",
     "shell.execute_reply.started": "2024-07-23T12:22:58.711029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"vqa_model_43\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"vqa_model_43\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ image_encoder_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ImageEncoder</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ language_encoder_43             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LanguageEncoder</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ insample_attention_43           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InsampleAttention</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ cross_sample_attention_16       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CrossSampleAttention</span>)          │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_358 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ image_encoder_43 (\u001b[38;5;33mImageEncoder\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ language_encoder_43             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mLanguageEncoder\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ insample_attention_43           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mInsampleAttention\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ cross_sample_attention_16       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mCrossSampleAttention\u001b[0m)          │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_38 (\u001b[38;5;33mDecoder\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_358 (\u001b[38;5;33mDense\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_size =len(tokenizer.word_index)+1 \n",
    "emb_dims = 512 \n",
    "rnn_units = 128 \n",
    "head = 8 \n",
    "layer = 6\n",
    "model = VQAModel(vocab_size , emb_dims , rnn_units )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:22:59.885168Z",
     "iopub.status.busy": "2024-07-23T12:22:59.884872Z",
     "iopub.status.idle": "2024-07-23T12:22:59.903784Z",
     "shell.execute_reply": "2024-07-23T12:22:59.902863Z",
     "shell.execute_reply.started": "2024-07-23T12:22:59.885141Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, encoder_inputs, decoder_inputs, decoder_outputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        context_vector = model(encoder_inputs, decoder_inputs, training=True)\n",
    "        predictions = model.decode(context_vector, decoder_inputs, training=True)\n",
    "       \n",
    "        loss = loss_object(decoder_outputs, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_loss.update_state(loss)\n",
    "    train_accuracy.update_state(decoder_outputs, predictions)\n",
    "\n",
    "@tf.function\n",
    "def val_step(model, encoder_inputs, decoder_inputs, decoder_outputs):\n",
    "    context_vector = model(encoder_inputs, decoder_inputs, training=False)\n",
    "    predictions = model.decode(context_vector, decoder_inputs, training=False)\n",
    "    loss = loss_object(decoder_outputs, predictions)\n",
    "    \n",
    "    val_loss.update_state(loss)\n",
    "    val_accuracy.update_state(decoder_outputs, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:22:59.905585Z",
     "iopub.status.busy": "2024-07-23T12:22:59.904974Z",
     "iopub.status.idle": "2024-07-23T12:22:59.910397Z",
     "shell.execute_reply": "2024-07-23T12:22:59.909497Z",
     "shell.execute_reply.started": "2024-07-23T12:22:59.905551Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = CustomDataGenerator(train_df , tokenizer , pad_len)\n",
    "val_data = CustomDataGenerator(val_df , tokenizer , pad_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:39:06.296495Z",
     "iopub.status.busy": "2024-07-23T12:39:06.296092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77937fc9aac84097a4eebf2f5a5c4025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8570797b730349fca894457b9aa4f570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/77 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.0432, Accuracy: 61.35%, Val Loss: 2.0573, Val Accuracy: 61.50%\n",
      "Epoch 2 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819f9cb7e89548deaa66259c116ecc92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed717e333a84842a4bdd17210e20f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/77 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.0300, Accuracy: 61.32%, Val Loss: 2.0267, Val Accuracy: 61.42%\n",
      "Epoch 3 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fcf9bcf9fd415fa3b1c3a835e1dd0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed495cb27b3a43b198b455a8855bb390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/77 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.0077, Accuracy: 61.54%, Val Loss: 2.0128, Val Accuracy: 62.18%\n",
      "Epoch 4 / 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb3e3fb00e24e32b9d3889964330043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/311 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 5\n",
    "train_steps = len(train_data) + 1\n",
    "val_steps = len(val_data) + 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss.reset_state()\n",
    "    train_accuracy.reset_state()\n",
    "    val_loss.reset_state()\n",
    "    val_accuracy.reset_state()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} / {epochs}\")\n",
    "\n",
    "    train_tqdm = tqdm(train_data, total=len(train_data), desc=\"Training\", unit='batch')\n",
    "    for step, (inputs, target) in enumerate(train_tqdm):\n",
    "        encoder_inputs, decoder_inputs = inputs\n",
    "        train_step(model, encoder_inputs, decoder_inputs, target)\n",
    "        train_tqdm.set_postfix(loss=train_loss.result().numpy(), accuracy=train_accuracy.result().numpy() * 100)\n",
    "        if step + 1 >= train_steps:\n",
    "            break\n",
    "\n",
    "    val_data_tqdm = tqdm(val_data, total=len(val_data), desc=\"Validation\", unit='batch')\n",
    "    for step, (inputs, target) in enumerate(val_data_tqdm):\n",
    "        val_encoder_inputs, val_decoder_inputs = inputs\n",
    "        val_step(model, val_encoder_inputs, val_decoder_inputs, target)\n",
    "        val_data_tqdm.set_postfix(loss=val_loss.result().numpy(), accuracy=val_accuracy.result().numpy() * 100)\n",
    "        if step + 1 >= val_steps:\n",
    "            break\n",
    "\n",
    "    template = \"Epoch {} Loss: {:.4f}, Accuracy: {:.2f}%, Val Loss: {:.4f}, Val Accuracy: {:.2f}%\"\n",
    "    print(template.format(epoch + 1,\n",
    "                          train_loss.result().numpy(),\n",
    "                          train_accuracy.result().numpy() * 100,\n",
    "                          val_loss.result().numpy(),\n",
    "                          val_accuracy.result().numpy() * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"knr.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:37:11.937047Z",
     "iopub.status.busy": "2024-07-23T12:37:11.936671Z",
     "iopub.status.idle": "2024-07-23T12:37:12.936056Z",
     "shell.execute_reply": "2024-07-23T12:37:12.934901Z",
     "shell.execute_reply.started": "2024-07-23T12:37:11.937017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 256)\n",
      "(1, 256)\n",
      "(1, 512)\n",
      "(1, 1, 1)\n",
      "(1, 512, 512)\n",
      "(1, 1, 2)\n",
      "(1, 512, 512)\n",
      "(1, 1, 3)\n",
      "(1, 512, 512)\n",
      "(1, 1, 4)\n",
      "(1, 512, 512)\n",
      "(1, 1, 5)\n",
      "(1, 512, 512)\n",
      "(1, 1, 6)\n",
      "(1, 512, 512)\n",
      "(1, 1, 7)\n",
      "(1, 512, 512)\n",
      "(1, 1, 8)\n",
      "(1, 512, 512)\n",
      "(1, 1, 9)\n",
      "(1, 512, 512)\n",
      "(1, 1, 10)\n",
      "(1, 512, 512)\n",
      "(1, 1, 11)\n",
      "(1, 512, 512)\n",
      "(1, 1, 12)\n",
      "(1, 512, 512)\n",
      "(1, 1, 13)\n",
      "(1, 512, 512)\n",
      "(1, 1, 14)\n",
      "(1, 512, 512)\n",
      "(1, 1, 15)\n",
      "(1, 512, 512)\n",
      "(1, 1, 16)\n",
      "(1, 512, 512)\n",
      "(1, 1, 17)\n",
      "(1, 512, 512)\n",
      "(1, 1, 18)\n",
      "(1, 512, 512)\n",
      "(1, 1, 19)\n",
      "(1, 512, 512)\n",
      "(1, 1, 20)\n",
      "(1, 512, 512)\n",
      "(1, 1, 21)\n",
      "(1, 512, 512)\n",
      "(1, 1, 22)\n",
      "(1, 512, 512)\n",
      "(1, 1, 23)\n",
      "(1, 512, 512)\n",
      "(1, 1, 24)\n",
      "(1, 512, 512)\n",
      "(1, 1, 25)\n",
      "(1, 512, 512)\n",
      "(1, 1, 26)\n",
      "(1, 512, 512)\n",
      "(1, 1, 27)\n",
      "(1, 512, 512)\n",
      "(1, 1, 28)\n",
      "(1, 512, 512)\n",
      "(1, 1, 29)\n",
      "(1, 512, 512)\n",
      "(1, 1, 30)\n",
      "(1, 512, 512)\n",
      "(1, 1, 31)\n",
      "(1, 512, 512)\n",
      "(1, 1, 32)\n",
      "(1, 512, 512)\n",
      "(1, 1, 33)\n",
      "(1, 512, 512)\n",
      "(1, 1, 34)\n",
      "(1, 512, 512)\n",
      "(1, 1, 35)\n",
      "(1, 512, 512)\n",
      "(1, 1, 36)\n",
      "(1, 512, 512)\n",
      "(1, 1, 37)\n",
      "(1, 512, 512)\n",
      "(1, 1, 38)\n",
      "(1, 512, 512)\n",
      "['but', 'most', 'game_table', 'what', 'shower_hose', 'there', 'refrigerator', 'what', 'how', 'package_of_bottled_water', 'what', '7', 'knife_rack,', 'beds', '10', 'any', 'what', 'what', 'glass_rack', 'game_table', 'poster', 'flatbed_scanner', 'beds', 'what', 'the', 'what', 'what', '', 'bedside', 'filled', 'tallest', 'displaying', 'type', 'what', 'cobalt', 'pairs', 'paper_cutter,', '']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_image(image_path, new_shape=(224, 224)):\n",
    "    image_data = cv2.imread(image_path)\n",
    "    image_data = cv2.resize(image_data, new_shape)\n",
    "    image_data = image_data / 255.0  \n",
    "    return image_data\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_answer(model, image, question, start_token, end_token, max_length=38):\n",
    "    context_vector = model(image[tf.newaxis], question[tf.newaxis])  # Add batch dimension\n",
    "\n",
    "    decoder_inputs = tf.fill([1, 1], start_token) \n",
    "\n",
    "    predicted_tokens = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        predictions = model.decode(context_vector, decoder_inputs) \n",
    "        \n",
    "        next_token_logits = predictions[:, -1, :]  \n",
    "        next_token = tf.argmax(next_token_logits, axis=-1) \n",
    "        next_token = tf.cast(next_token, tf.int32)\n",
    "\n",
    "        predicted_tokens.append(next_token[0].numpy())\n",
    "\n",
    "        decoder_inputs = tf.concat([decoder_inputs, next_token[:, tf.newaxis]], axis=1)\n",
    "\n",
    "        if next_token[0].numpy() == end_token:\n",
    "            break\n",
    "\n",
    "    return predicted_tokens\n",
    "\n",
    "start_token = 3134  \n",
    "end_token = 3135  \n",
    "\n",
    "processed_image = process_image(\"/kaggle/input/visual-question-answering-computer-vision-nlp/dataset/images/image1.png\")\n",
    "\n",
    "question = \"What is on the left side of the white oven on the floor and on right side of the blue armchair in the image1?\"\n",
    "tokenized_question = tokenize(question, tokenizer, 38)\n",
    "\n",
    "predicted_tokens = generate_answer(model, processed_image, tokenized_question, start_token, end_token)\n",
    "\n",
    "\n",
    "predicted_text =[detokenize_sequence(tokenizer,pre)for pre in predicted_tokens] \n",
    "print(predicted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2264789,
     "sourceId": 3798293,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5429445,
     "sourceId": 9011414,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
